"""
s5_LegalSearchEngine.py
í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(ë²¡í„° + BM25) + ë²•ë ¹ íŠ¹í™” ê¸°ëŠ¥
"""

import numpy as np
import faiss
from typing import List, Dict, Optional
from rank_bm25 import BM25Okapi
import re
import json


"""
BM25Okapi
BM25OkapiëŠ” ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë­í‚¹ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
ê²€ìƒ‰ì–´(query)ì™€ ë¬¸ì„œë“¤ ê°„ì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤
TF-IDFì˜ ê°œì„ ëœ ë²„ì „ìœ¼ë¡œ, ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™”ì™€ ë‹¨ì–´ ë¹ˆë„ í¬í™”ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤
(BM25ëŠ” í˜„ì‹¤ì ì¸ ê²€ìƒ‰ì„ ìœ„í•´ "ë¬¸ì„œ ê¸¸ì´"ì™€ "ê³¼ë„í•œ ë°˜ë³µ"ì˜ ì˜í–¥ì„ ì¡°ì ˆí•´ì„œ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤)

# 1. BM25 ëª¨ë¸ ìƒì„±
bm25 = BM25Okapi(tokenized_corpus)

# 2. ê²€ìƒ‰ì–´ë¡œ ë¬¸ì„œ ì ìˆ˜ ê³„ì‚°
query = "ì•ˆì „ ê·œì •"
tokenized_query = query.split()

scores = bm25.get_scores(tokenized_query)
# ì¶œë ¥: [1.52, 0.93, 0.81]  # ê° ë¬¸ì„œì˜ ê´€ë ¨ì„± ì ìˆ˜

# 3. ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ì°¾ê¸°
top_doc = bm25.get_top_n(tokenized_query, corpus, n=1)
# ì¶œë ¥: ['ê±´ì„¤ ì•ˆì „ ê´€ë¦¬ ê·œì •ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤']
"""

"""
bm25 = BM25Okapi(corpus)
```

ì´ í•œ ì¤„ì´ í•˜ëŠ” ì¼:
```
1ï¸âƒ£ ë¬¸ì„œ ë¹ˆë„ (DF) ê³„ì‚°
   - "ë¹„ê³„"ê°€ ëª‡ ê°œ ë¬¸ì„œì— ë“±ì¥? â†’ 127ê°œ
   - "ì•ˆì „"ì´ ëª‡ ê°œ ë¬¸ì„œì— ë“±ì¥? â†’ 892ê°œ
   - "ì œ57ì¡°"ê°€ ëª‡ ê°œ ë¬¸ì„œì— ë“±ì¥? â†’ 3ê°œ

2ï¸âƒ£ ì—­ë¬¸ì„œ ë¹ˆë„ (IDF) ê³„ì‚°
   - IDF("ë¹„ê³„") = log(1500/127) = 2.47
   - IDF("ì•ˆì „") = log(1500/892) = 0.52  â† í”í•œ ë‹¨ì–´ë¼ ë‚®ìŒ
   - IDF("ì œ57ì¡°") = log(1500/3) = 6.21  â† í¬ê·€í•´ì„œ ë†’ìŒ

3ï¸âƒ£ í‰ê·  ë¬¸ì„œ ê¸¸ì´ ê³„ì‚°
   - avgdl = ì „ì²´ í† í° ìˆ˜ / ë¬¸ì„œ ìˆ˜
   
4ï¸âƒ£ ê° ë¬¸ì„œì˜ ê¸¸ì´ ì €ì¥
   - doc_lens = [350, 420, 380, ...]
"""

class LegalSearchEngine:
    """ë²•ë ¹ íŠ¹í™” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(self, 
                 faiss_index: faiss.Index,
                 metadata: List[Dict],  
                 embedding_manager=None):
        """
        Args:
            faiss_index: FAISS ì¸ë±ìŠ¤
            metadata: ë©”íƒ€ë°ì´í„° (chunks ì •ë³´ í¬í•¨)
            embedding_manager: EmbeddingManager ì¸ìŠ¤í„´ìŠ¤
        """
        self.faiss_index = faiss_index
        self.metadata = metadata
        self.embedding_manager = embedding_manager
        
        # BM25 ì¸ë±ìŠ¤ ìƒì„±
        print("\nğŸ”§ BM25 ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        self.build_bm25_index()
        
        print("\nâœ“ LegalSearchEngine ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  - FAISS ë²¡í„° ìˆ˜: {faiss_index.ntotal}")
        print(f"  - BM25 ë¬¸ì„œ ìˆ˜: {len(self.bm25_corpus)}")
    
    def tokenize_korean(self, text: str) -> List[str]:
        """
        í•œê¸€ í…ìŠ¤íŠ¸ í† í°í™”
        ì´ í•¨ìˆ˜ëŠ” í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ëŠ” í† í°í™” í•¨ìˆ˜ì…ë‹ˆë‹¤.
        ì •ê·œí‘œí˜„ì‹ \w+ë¥¼ ì‚¬ìš©í•´ì„œ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤
        ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤ (.lower())
        ë‹¨ì–´ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤
        """
        tokens = re.findall(r'\w+', text.lower())
        return tokens
    
    def build_bm25_index(self):
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶• (metadataì—ì„œ ì§ì ‘)"""
        self.bm25_corpus = []
        
        for item in self.metadata:
            content = item.get('content', '')
            tokens = self.tokenize_korean(content)
            self.bm25_corpus.append(tokens)
        
        self.bm25 = BM25Okapi(self.bm25_corpus)
        print(f"  âœ“ BM25 ì¸ë±ìŠ¤: {len(self.bm25_corpus)}ê°œ ë¬¸ì„œ")
    
    def contains_article(self, content: str, article: str) -> bool:
        """
        ì²­í¬ ë‚´ìš©ì— íŠ¹ì • ì¡°(æ¢)ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        
        Args:
            content: ì²­í¬ ë‚´ìš©
            article: ì¡° ë²ˆí˜¸ (ì˜ˆ: "ì œ36ì¡°")
        
        Returns:
            í¬í•¨ ì—¬ë¶€
        """
        # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì •í™•íˆ ë§¤ì¹­
        pattern = re.escape(article) + r'(?:\s|[^\wê°€-í£]|$)'
        return bool(re.search(pattern, content))
    
    def contains_chapter(self, content: str, chapter: str) -> bool:
        """ì²­í¬ ë‚´ìš©ì— íŠ¹ì • ì¥(ç« )ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        pattern = re.escape(chapter) + r'(?:\s|[^\wê°€-í£]|$)'
        return bool(re.search(pattern, content))
    
    def vector_search(self, 
                     query: str,
                     top_k: int = 10,
                     filter_article: Optional[str] = None,
                     filter_chapter: Optional[str] = None) -> List[Dict]:
        """
        ë²¡í„° ê²€ìƒ‰ + ë²•ë ¹ í•„í„°ë§ (í…ìŠ¤íŠ¸ ê¸°ë°˜)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            filter_article: ì¡° í•„í„° (ì˜ˆ: "ì œ36ì¡°") - ì²­í¬ ë‚´ìš©ì—ì„œ ê²€ìƒ‰
            filter_chapter: ì¥ í•„í„° (ì˜ˆ: "ì œ2ì¥") - ì²­í¬ ë‚´ìš©ì—ì„œ ê²€ìƒ‰
        """
        if not self.embedding_manager:
            raise ValueError("EmbeddingManagerê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_manager.embed_text(query)
        query_embedding = query_embedding.reshape(1, -1).astype('float32')
        
        # FAISS ê²€ìƒ‰ (í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ê°€ì ¸ì˜´)
        search_k = top_k * 10 if (filter_article or filter_chapter) else top_k
        distances, indices = self.faiss_index.search(query_embedding, search_k)
        
        # ê²°ê³¼ êµ¬ì„± ë° í•„í„°ë§
        results = []
        for idx, distance in zip(indices[0], distances[0]):
            if idx >= len(self.metadata):
                continue
            
            item = self.metadata[idx]
            content = item["content"]
            
            # í…ìŠ¤íŠ¸ ê¸°ë°˜ í•„í„°ë§
            if filter_article and not self.contains_article(content, filter_article):
                continue
            
            if filter_chapter and not self.contains_chapter(content, filter_chapter):
                continue
            
            result = {
                "rank": len(results) + 1,
                "chunk_id": item["chunk_id"],
                "content": content,
                "metadata": item["metadata"],
                "score": float(1 / (1 + distance)),
                "search_type": "vector"
            }
            results.append(result)
            
            if len(results) >= top_k:
                break
        
        return results
    
    def keyword_search(self,
                      query: str,
                      top_k: int = 10,
                      filter_article: Optional[str] = None,
                      filter_chapter: Optional[str] = None) -> List[Dict]:
        """
        í‚¤ì›Œë“œ ê²€ìƒ‰ + ë²•ë ¹ í•„í„°ë§ (í…ìŠ¤íŠ¸ ê¸°ë°˜)
        """
        query_tokens = self.tokenize_korean(query)
        scores = self.bm25.get_scores(query_tokens)
        
        # ìŠ¤ì½”ì–´ ì •ë ¬
        ranked_indices = np.argsort(scores)[::-1]
        
        # ê²°ê³¼ êµ¬ì„± ë° í•„í„°ë§
        results = []
        for idx in ranked_indices:
            if scores[idx] <= 0:
                continue
            
            item = self.metadata[idx]
            content = item["content"]
            
            # í…ìŠ¤íŠ¸ ê¸°ë°˜ í•„í„°ë§
            if filter_article and not self.contains_article(content, filter_article):
                continue
            
            if filter_chapter and not self.contains_chapter(content, filter_chapter):
                continue
            
            result = {
                "rank": len(results) + 1,
                "chunk_id": item["chunk_id"],
                "content": content,
                "metadata": item["metadata"],
                "score": float(scores[idx]),
                "search_type": "keyword"
            }
            results.append(result)
            
            if len(results) >= top_k:
                break
        
        return results
    
    def reciprocal_rank_fusion(self,
                               vector_results: List[Dict],
                               keyword_results: List[Dict],
                               k: int = 60) -> List[Dict]:
        """RRF ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê²°ê³¼ ìœµí•©"""
        chunk_scores = {}
        chunk_data = {}
        
        for result in vector_results:
            chunk_id = result["chunk_id"]
            rank = result["rank"]
            rrf_score = 1 / (k + rank)
            
            chunk_scores[chunk_id] = chunk_scores.get(chunk_id, 0) + rrf_score
            chunk_data[chunk_id] = result
        
        for result in keyword_results:
            chunk_id = result["chunk_id"]
            rank = result["rank"]
            rrf_score = 1 / (k + rank)
            
            chunk_scores[chunk_id] = chunk_scores.get(chunk_id, 0) + rrf_score
            if chunk_id not in chunk_data:
                chunk_data[chunk_id] = result
        
        sorted_chunks = sorted(chunk_scores.items(), key=lambda x: x[1], reverse=True)
        
        results = []
        for i, (chunk_id, score) in enumerate(sorted_chunks):
            result = chunk_data[chunk_id].copy()
            result["rank"] = i + 1
            result["rrf_score"] = float(score)
            result["search_type"] = "hybrid"
            results.append(result)
        
        return results
    
    def hybrid_search(self,
                     query: str,
                     top_k: int = 10,
                     filter_article: Optional[str] = None,
                     filter_chapter: Optional[str] = None) -> List[Dict]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ë²•ë ¹ í•„í„°ë§
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            filter_article: ì¡° í•„í„° (ì˜ˆ: "ì œ36ì¡°")
            filter_chapter: ì¥ í•„í„° (ì˜ˆ: "ì œ2ì¥")
        """
        # ë²¡í„° ê²€ìƒ‰
        vector_results = self.vector_search(
            query, 
            top_k=top_k*2,
            filter_article=filter_article,
            filter_chapter=filter_chapter
        )
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰
        keyword_results = self.keyword_search(
            query,
            top_k=top_k*2,
            filter_article=filter_article,
            filter_chapter=filter_chapter
        )
        
        # RRF ìœµí•©
        hybrid_results = self.reciprocal_rank_fusion(vector_results, keyword_results)
        
        return hybrid_results[:top_k]
    
def main():
    """í…ŒìŠ¤íŠ¸ ì½”ë“œ"""
    import os
    from s4_EmbeddingManager import EmbeddingManager
    from dotenv import load_dotenv
    
    print("="*80)
    print("ğŸ” ë²•ë ¹ íŠ¹í™” ê²€ìƒ‰ì—”ì§„ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    
    if not OPENAI_API_KEY:
        print("\nâœ— ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
    current_dir = os.path.dirname(os.path.abspath(__file__))  # src/
    project_root = os.path.dirname(current_dir)  # CNTWORKBOT_PJT1/
    
    # ê²½ë¡œ ì„¤ì • (ì ˆëŒ€ ê²½ë¡œ)
    vector_store_dir = os.path.join(project_root, "data", "vector_store", "construction_law")
    cache_dir = os.path.join(project_root, "data", "cache")
    
    index_path = os.path.join(vector_store_dir, "faiss_index.bin")
    metadata_path = os.path.join(vector_store_dir, "metadata.json")
    
    print(f"\ní”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    print(f"ë²¡í„° ì €ì¥ì†Œ: {vector_store_dir}")
    print(f"ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
    
    # EmbeddingManager ì´ˆê¸°í™”
    em = EmbeddingManager(
        openai_api_key=OPENAI_API_KEY,
        institution="construction_law",
        cache_dir=cache_dir 
    )
    
    # ì¸ë±ìŠ¤ ë¡œë“œ
    index = em.load_index(index_path)
    metadata = em.load_metadata(metadata_path)
    
    if index is None or metadata is None:
        print("\nâœ— ì¸ë±ìŠ¤ ë˜ëŠ” ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € s4_EmbeddingManager.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    # SearchEngine ì´ˆê¸°í™”
    search_engine = LegalSearchEngine(
        faiss_index=index,
        metadata=metadata,
        embedding_manager=em
    )
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    print("\n" + "="*80)
    print("ğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬")
    print("="*80)
    
    query = "ê±´íìœ¨ì€ ì–´ë–»ê²Œ ê³„ì‚°í•˜ë‚˜ìš”?"
    print(f"\nì¿¼ë¦¬: {query}")
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    results = search_engine.hybrid_search(query, top_k=5)
    
    print(f"\nê²€ìƒ‰ ê²°ê³¼: {len(results)}ê±´\n")
    for result in results:
        print(f"\n[{result['rank']}] {result['chunk_id']}")
        print(f"ë©”íƒ€ë°ì´í„°:")
        print(json.dumps(result['metadata'], indent=2, ensure_ascii=False))
        print(f"\në‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {result['content'][:2000]}...")
        print("-" * 80)

if __name__ == "__main__":
    main()