"""
s4_EmbeddingManager.py
ì„ë² ë”© ìƒì„±ê³¼ FAISS ì¸ë±ìŠ¤ ê´€ë¦¬
"""

"""
âœ… FAISS ì¸ë±ìŠ¤ëŠ” "ë²¡í„° ì°½ê³ "
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FAISS ì¸ë±ìŠ¤ = 1500ê°œ ë²¡í„°ë¥¼ ê·¸ëƒ¥ ì €ì¥í•´ë‘” ì°½ê³         â”‚
â”‚                                                         â”‚
â”‚ ğŸ“¦ ë²¡í„° 0:    [0.023, -0.056, 0.089, ...]              â”‚
â”‚ ğŸ“¦ ë²¡í„° 1:    [0.045, 0.012, -0.034, ...]              â”‚
â”‚ ğŸ“¦ ë²¡í„° 2:    [-0.078, 0.091, 0.056, ...]              â”‚
â”‚ ...                                                     â”‚
â”‚ ğŸ“¦ ë²¡í„° 1499: [0.034, -0.067, 0.045, ...]              â”‚
â”‚                                                         â”‚
â”‚ â†’ ë¯¸ë¦¬ ì •ë ¬ë˜ì–´ ìˆì§€ ì•ŠìŒ!                             â”‚
â”‚ â†’ ê²€ìƒ‰í•  ë•Œ ê±°ë¦¬ ê³„ì‚°í•´ì„œ ì •ë ¬í•¨                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

import os
import json
import pickle
import hashlib
from typing import List, Dict, Optional, Tuple
import numpy as np
from openai import OpenAI
import faiss
from dotenv import load_dotenv


class EmbeddingManager:
    """ì„ë² ë”© ìƒì„± ë° FAISS ì¸ë±ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 openai_api_key: str,
                 institution: str = "construction_law",
                 model: str = "text-embedding-3-large",
                 cache_dir: str = None,
                 dimension: int = 3072):
        """
        Args:
            openai_api_key: OpenAI API í‚¤
            institution: ê¸°ê´€/í”„ë¡œì íŠ¸ ì´ë¦„ (ìºì‹œ íŒŒì¼ëª…ìš©)
            model: ì„ë² ë”© ëª¨ë¸ëª…
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œ ê¶Œì¥, Noneì´ë©´ "data/cache")
            dimension: ì„ë² ë”© ì°¨ì›
        """
        self.client = OpenAI(api_key=openai_api_key)
        self.model = model
        self.institution = institution
        self.dimension = dimension
        
        # ìºì‹œ ê²½ë¡œ ìë™ ìƒì„±
        if cache_dir is None:
            cache_dir = "data/cache"
        
        os.makedirs(cache_dir, exist_ok=True)
        cache_path = os.path.join(cache_dir, f"embeddings_{institution}.pkl")
        
        self.cache_path = cache_path
        self.embedding_cache = self.load_embedding_cache()
        
        print(f"ğŸ¯ EmbeddingManager ì´ˆê¸°í™”")
        print(f"  - ëª¨ë¸: {model}")
        print(f"  - ì°¨ì›: {dimension}")
        print(f"  - ìºì‹œ: {len(self.embedding_cache)}ê°œ ì„ë² ë”©")
        print(f"  - ìºì‹œ ê²½ë¡œ: {cache_path}")
    
    def load_embedding_cache(self) -> Dict[str, np.ndarray]:
        """ì„ë² ë”© ìºì‹œ ë¡œë“œ"""
        if os.path.exists(self.cache_path):
            try:
                with open(self.cache_path, 'rb') as f:
                    cache = pickle.load(f)
                print(f"âœ“ ìºì‹œ ë¡œë“œ: {len(cache)}ê°œ ì„ë² ë”©")
                return cache
            except Exception as e:
                print(f"âš  ìºì‹œ ë¡œë“œ ì‹¤íŒ¨ ({e}), ìƒˆë¡œ ì‹œì‘")
                return {}
        return {}
    
    def save_embedding_cache(self):
        """ì„ë² ë”© ìºì‹œ ì €ì¥"""
        try:
            with open(self.cache_path, 'wb') as f:
                pickle.dump(self.embedding_cache, f)
            print(f"âœ“ ìºì‹œ ì €ì¥: {len(self.embedding_cache)}ê°œ ì„ë² ë”©")
        except Exception as e:
            print(f"âš  ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_text_hash(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì˜ MD5 í•´ì‹œ ê³„ì‚° (ìºì‹œ í‚¤)"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def embed_text(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        # ìºì‹œ í™•ì¸
        text_hash = self.get_text_hash(text)
        if text_hash in self.embedding_cache:
            return self.embedding_cache[text_hash]
        
        # OpenAI API í˜¸ì¶œ
        try:
            response = self.client.embeddings.create(
                input=text,
                model=self.model
            )
            
            embedding = np.array(response.data[0].embedding, dtype='float32')
            
            # ìºì‹œì— ì €ì¥
            self.embedding_cache[text_hash] = embedding
            
            return embedding
            
        except Exception as e:
            print(f"âš  ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros(self.dimension, dtype='float32')
    
    def embed_chunks(self, chunks: List[Dict], batch_size: int = 100) -> Tuple[List[np.ndarray], List[str]]:
        """
        ì—¬ëŸ¬ ì²­í¬ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”©
        
        Returns:
            (ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸, ì²­í¬ ID ë¦¬ìŠ¤íŠ¸)
        """
        embeddings = []
        chunk_ids = []
        
        print(f"\nğŸ§® ì„ë² ë”© ìƒì„± ì‹œì‘...")
        print(f"  - ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
        print(f"  - ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        cache_hits = 0
        cache_misses = 0
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]
            batch_texts = [chunk['content'] for chunk in batch]
            batch_chunk_ids = [chunk['chunk_id'] for chunk in batch]
            
            print(f"\n  ë°°ì¹˜ {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1}")
            
            # ë°°ì¹˜ ë‚´ì—ì„œ ìºì‹œ í™•ì¸
            batch_embeddings = []
            texts_to_embed = []
            text_indices = []
            
            for j, text in enumerate(batch_texts):
                text_hash = self.get_text_hash(text)
                if text_hash in self.embedding_cache:
                    batch_embeddings.append(self.embedding_cache[text_hash])
                    cache_hits += 1
                else:
                    batch_embeddings.append(None)
                    texts_to_embed.append(text)
                    text_indices.append(j)
                    cache_misses += 1
            
            # ìºì‹œì— ì—†ëŠ” ê²ƒë§Œ API í˜¸ì¶œ
            if texts_to_embed:
                try:
                    response = self.client.embeddings.create(
                        input=texts_to_embed,
                        model=self.model
                    )
                    
                    for idx, data in enumerate(response.data):
                        embedding = np.array(data.embedding, dtype='float32')
                        original_idx = text_indices[idx]
                        batch_embeddings[original_idx] = embedding
                        
                        # ìºì‹œì— ì €ì¥
                        text_hash = self.get_text_hash(texts_to_embed[idx])
                        self.embedding_cache[text_hash] = embedding
                    
                    print(f"    âœ“ {len(texts_to_embed)}ê°œ ìƒˆë¡œ ìƒì„±")
                    
                except Exception as e:
                    print(f"    âœ— ë°°ì¹˜ ì‹¤íŒ¨: {e}")
                    for idx in text_indices:
                        if batch_embeddings[idx] is None:
                            batch_embeddings[idx] = np.zeros(self.dimension, dtype='float32')
            
            embeddings.extend(batch_embeddings)
            chunk_ids.extend(batch_chunk_ids)
            
            progress = min((i + batch_size) / len(chunks) * 100, 100)
            print(f"    ì§„í–‰ë¥ : {progress:.1f}%")
        
        print(f"\nâœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
        print(f"  - ìºì‹œ íˆíŠ¸: {cache_hits}ê°œ")
        print(f"  - ìƒˆë¡œ ìƒì„±: {cache_misses}ê°œ")
        
        if cache_misses > 0:
            self.save_embedding_cache()
        
        return embeddings, chunk_ids
    
    def create_faiss_index(self, embeddings: List[np.ndarray]) -> faiss.Index:
        """FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        print(f"\nğŸ”§ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        # Flat ì¸ë±ìŠ¤ (ì •í™•í•œ ìµœê·¼ì ‘ ì´ì›ƒ ê²€ìƒ‰)
        index = faiss.IndexFlatL2(self.dimension)
        
        # numpy ë°°ì—´ë¡œ ë³€í™˜
        embeddings_array = np.array(embeddings).astype('float32')
        
        # ì¸ë±ìŠ¤ì— ì¶”ê°€
        index.add(embeddings_array)
        
        print(f"âœ“ FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        print(f"  - íƒ€ì…: Flat (L2)")
        print(f"  - ë²¡í„° ìˆ˜: {index.ntotal}")
        
        return index
    
    def save_index(self, index: faiss.Index, index_path: str):
        """FAISS ì¸ë±ìŠ¤ ì €ì¥"""
        os.makedirs(os.path.dirname(index_path), exist_ok=True)
        
        try:
            faiss.write_index(index, index_path)
            print(f"âœ“ ì¸ë±ìŠ¤ ì €ì¥: {index_path}")
        except Exception as e:
            print(f"âœ— ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_index(self, index_path: str) -> Optional[faiss.Index]:
        """FAISS ì¸ë±ìŠ¤ ë¡œë“œ"""
        if not os.path.exists(index_path):
            print(f"âš  ì¸ë±ìŠ¤ ì—†ìŒ: {index_path}")
            return None
        
        try:
            index = faiss.read_index(index_path)
            print(f"âœ“ ì¸ë±ìŠ¤ ë¡œë“œ: {index_path}")
            print(f"  - ë²¡í„° ìˆ˜: {index.ntotal}")
            return index
        except Exception as e:
            print(f"âœ— ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def save_metadata(self, chunks: List[Dict], chunk_ids: List[str], metadata_path: str):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        os.makedirs(os.path.dirname(metadata_path), exist_ok=True)
        
        chunk_dict = {chunk['chunk_id']: chunk for chunk in chunks}
        
        metadata = []
        for i, chunk_id in enumerate(chunk_ids):
            chunk = chunk_dict.get(chunk_id, {})
            metadata.append({
                "index": i,
                "chunk_id": chunk_id,
                "content": chunk.get("content", ""),
                "metadata": chunk.get("metadata", {})
            })
        
        try:
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            print(f"âœ“ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
            print(f"  - í•­ëª© ìˆ˜: {len(metadata)}")
        except Exception as e:
            print(f"âœ— ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_metadata(self, metadata_path: str) -> Optional[List[Dict]]:
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if not os.path.exists(metadata_path):
            print(f"âš  ë©”íƒ€ë°ì´í„° ì—†ìŒ: {metadata_path}")
            return None
        
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            print(f"âœ“ ë©”íƒ€ë°ì´í„° ë¡œë“œ: {metadata_path}")
            print(f"  - í•­ëª© ìˆ˜: {len(metadata)}")
            return metadata
        except Exception as e:
            print(f"âœ— ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def build_index_from_chunks(self, chunks_path: str, 
                                output_dir: str = None) -> Tuple[faiss.Index, List[Dict]]:
        """
        ì²­í¬ íŒŒì¼ì—ì„œ ì¸ë±ìŠ¤ êµ¬ì¶• (ì „ì²´ íŒŒì´í”„ë¼ì¸)
        
        Returns:
            (FAISS ì¸ë±ìŠ¤, ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸)
        """
        if output_dir is None:
            output_dir = f"data/vector_store/{self.institution}"
        
        print("\n" + "="*80)
        print("ğŸš€ FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘")
        print("="*80)
        
        # 1. ì²­í¬ ë¡œë“œ
        print("\n[1] ì²­í¬ ë¡œë“œ")
        with open(chunks_path, 'r', encoding='utf-8') as f:
            chunks = json.load(f)
        print(f"âœ“ {len(chunks)}ê°œ ì²­í¬")
        
        # 2. ì„ë² ë”© ìƒì„±
        print("\n[2] ì„ë² ë”© ìƒì„±")
        embeddings, chunk_ids = self.embed_chunks(chunks, batch_size=100)
        
        # 3. FAISS ì¸ë±ìŠ¤ ìƒì„±
        print("\n[3] FAISS ì¸ë±ìŠ¤ ìƒì„±")
        index = self.create_faiss_index(embeddings)
        
        # 4. ì €ì¥
        print("\n[4] ì €ì¥")
        index_path = os.path.join(output_dir, "faiss_index.bin")
        self.save_index(index, index_path)
        
        metadata_path = os.path.join(output_dir, "metadata.json")
        self.save_metadata(chunks, chunk_ids, metadata_path)
        
        metadata = self.load_metadata(metadata_path)
        
        print("\n" + "="*80)
        print("âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        print("="*80)
        print(f"ğŸ“ ì¶œë ¥: {output_dir}")
        print("="*80 + "\n")
        
        return index, metadata


def main():
    """
    EmbeddingManager ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜
    
    ì…ë ¥: data/chunks/construction_law_chunks.json
    ì¶œë ¥: data/vector_store/construction_law/
           - faiss_index.bin
           - metadata.json
    ìºì‹œ: data/cache/embeddings_construction_law.pkl
    """
    print("="*80)
    print("ğŸ§® ì„ë² ë”© ë° FAISS ì¸ë±ìŠ¤ êµ¬ì¶•")
    print("="*80)
    
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    openai_api_key = os.getenv("OPENAI_API_KEY")
    
    if not openai_api_key:
        print("\nâœ— ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("  .env íŒŒì¼ì— ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•´ì£¼ì„¸ìš”:")
        print("  OPENAI_API_KEY=sk-your-api-key-here")
        return
    
    # í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
    current_dir = os.path.dirname(os.path.abspath(__file__))  # src/
    project_root = os.path.dirname(current_dir)  # CNTCHATBOT_PJT2/
    
    # ê²½ë¡œ ì„¤ì • (ëª¨ë‘ ì ˆëŒ€ ê²½ë¡œ)
    CHUNKS_PATH = os.path.join(project_root, "data", "chunks", "construction_law_chunks.json")
    OUTPUT_DIR = os.path.join(project_root, "data", "vector_store", "construction_law")
    CACHE_DIR = os.path.join(project_root, "data", "cache")
 
    print(f"\ní”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    print(f"ì…ë ¥ íŒŒì¼: {CHUNKS_PATH}")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}")
    print(f"ìºì‹œ ë””ë ‰í† ë¦¬: {CACHE_DIR}")
    
    # ì…ë ¥ íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(CHUNKS_PATH):
        print(f"\nâœ— ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {CHUNKS_PATH}")
        print("ë¨¼ì € s3_LegalChunking.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    # ì´ë¯¸ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸° ì˜µì…˜
    index_path = os.path.join(OUTPUT_DIR, "faiss_index.bin")
    metadata_path = os.path.join(OUTPUT_DIR, "metadata.json")
    
    if os.path.exists(index_path) and os.path.exists(metadata_path):
        response = input(f"\nâš  ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {OUTPUT_DIR}\në®ì–´ì“°ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
        if response.lower() != 'y':
            print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
    
    # EmbeddingManager ì‹¤í–‰
    try:
        embedding_manager = EmbeddingManager(
            openai_api_key=openai_api_key,
            institution="construction_law",
            model="text-embedding-3-large",
            cache_dir=CACHE_DIR
        )
        
        index, metadata = embedding_manager.build_index_from_chunks(
            chunks_path=CHUNKS_PATH,
            output_dir=OUTPUT_DIR
        )
        
        print("="*80)
        print("âœ… FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        print("="*80)
        print(f"\nìƒì„±ëœ íŒŒì¼:")
        print(f"  - {os.path.join(OUTPUT_DIR, 'faiss_index.bin')}")
        print(f"  - {os.path.join(OUTPUT_DIR, 'metadata.json')}")
        print(f"  - {os.path.join(CACHE_DIR, 'embeddings_construction_law.pkl')}")
        print("="*80 + "\n")
        
    except Exception as e:
        print(f"\nâœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()